---
layout: post
title: CompSci 367 notes
category: uni
excerpt:
---

####Logic
- __Entail__ - _KB |= a_ if and only if _a_ is true in all worlds were KB is true
- __Infer__ - _KB |- i a_ = sentence _a_ can be derived from _KB_ by algorithm _i_
	- _i_ is __sound__ if whenever we infer something from _i_ it is also entailed by the KB
	- _i_ is __complete__ if something is entailed by the KB, then _i_ infers it
- __Conjunctive normal form__ (CNF): conjunction of disjunction of literals eg. _(A or B) and (B or C)_
- __Negation__ in prolog, when something cannot be proved true it is _false_

####Search
- __Complete__ - always finds a solution if one exists
- __Optimal__ - finds the _best_ solution
- _b_ - branching factor
- _d_ - depth of least cost solution
- _m_ - maximum depth of state space
- _f(n)_ is the _desirability_ of node _n_
- _g(n)_ is the cost from _initialState_ to _n_
- _h(n)_ is the estimate of the distance from _n_ to _goal_
	- __Admissible__ if the estimate is less than the actual cost (ie. never overestimates)
	- __Consistent__ if obeys the triangle inequality (ie. _h(n)_ is <= the cost from _n_ to _n'_ + _h(n')_)
	- a heuristic is likely to be better when its _average h-value_ (_r_) is higher
		-  __dominates__ another heuristic when _h1(n) > h2(n)_ for ALL _n_
	- uninformed tree = _b^d_, informed = _b^(d-r)_
	- heuristic can be formed from a __relaxed problem__ (reduce the restrictions)


| | Complete? | Time | Space | Optimal | _f(n)_ | Notes.. |
|:---|:---:|:---:|:---:|:---:|:---:|:---|
| BFS | Y (if _b_ finite) | Exponential | Exponential | Y (if step costs = 1) | | Uses a tonne of space (Graph search) |
| DFS | N (loops/infinite _m_) | Exponential | Linear (only keeps best in mem) | N | | |
| ID DFS | Y | Exponential | Linear (discard after each step) | Y (if step costs = 1) | | DFS + BFS advantages |
| Greedy | N | Exponential | Exponential | N | _h(n)_ | Doesn't care about _g(n)_ |
| A* | Y | Exponential | Exponential | Y (if _h_ is admissible + consistent) | _g(n) + h(n)_ | |
| IDA* | Y | Exponential | Linear | Y | _g(n) + h(n)_ | Iterates on the _f-limit_ - start with _h(init)_ |
| Weighted A* | Y | Exponential | Exponential | Y | _g(n) + h(n) * w_ | will be no worse than _w_ times as costly as optimal |
| Bidirectional A* Front-to-Back | Y | Exponential | Exponentially smaller (_b^(d/2)_) | Y | _g(n) + h(n)_ | _h(n)_ estimates diastase to the opposite terminal (init or goal), keeps searching for optimal |
| Bidirectional A* Front-to-Front | Y | Exponential | Exponentially smaller (_b^(d/2)_) | Y | _g(n) + h(n)_ | _h(n)_ estimates diastase to the opposite frontier, optimal on first collision!, cost of computing _h_ grow exponentially |
| Min-Max | Y | Exponential | Linear | Y (against optimal opponent) | _minmax value_ | |
| Min-Max _a-b_ pruning | Y | Exponentially smaller (_b^(m/2)_) | Linear | .. | _minmax value_ | _doubles_ depth which can be done in the same time |

- __monte Carlo Tree Search__ plays out the rest of a game randomly, thousands of times, picks the one with the highest win rate. (explores the whole problem space)

####Local Search
- __Hill climbing__ algorithm
	- always moves toward a maxima - can get stuck on a local maxima, missing the global maximum
	- add _random-restarts_ when you get stuck - trivially complete (depends on the shape of the state-space landscape)
- __Simulated annealing__ search - allows some 'bad' moves to escape local maxima, but _gradually decreases_ their size and frequency
- __Local beam__ search
	- start with _k_ randomly generated states
	- better than random restarts as information is shared -- but can get concentrated in a small region
- __Stochastic beam__ search - choose _k_ successors randomly, biased towards good ones (natural selection)

####Genetic algorithms
- requires: reproduction, population, variety, difference in ability to survive
- reproduce with a _probability proportional to their fitness_
- __overcrowding__ can happen if one individual is too dominant


####Constraint satisfaction problems (CSP)
- _state_ is defined by __variables__ _Xi_ with _values_ from __domain__ _Di_
- _goal test_ is a set of __constraints__ specifying allowable combinations of values for subsets of variables
	- __Unary__ constraints involve a single variable (_X != 5_)
	- __Binary__ constraints involve variable pairs (_X != Y_)
	- __Higher-order__ 3 or more variables
- __Incremental search__ - assign values to variables that do not conflict with current assignment
- __Backtracking__ search (DFS)
	- choose the most constrained variable ie. with the fewest legal values (_minimum remaining values (MRV)_ heuristic)
	- __Forward checking__ - keep track of legal values for all variables, abort when a variable has none remaining
	- __Arc consistency__ - also keeps track of the affect that forward checking has on neighbours (detects failure earlier)	
